{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Get the code from gihtub repo**","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/iiduka-researches/202009-coba/","metadata":{"_uuid":"5abc833c-9d16-43d2-9408-383eea335427","_cell_guid":"d3d46fb5-33b7-4d30-87f0-cdcb14a2d74b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Rename the repo name to a simple one**","metadata":{}},{"cell_type":"code","source":"mv 202009-coba coba","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Go into the directory**","metadata":{}},{"cell_type":"code","source":"cd coba","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Remove some extra files**","metadata":{}},{"cell_type":"code","source":"rm requirements_aws.txt download_coco.sh mount.sh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **List files**","metadata":{}},{"cell_type":"code","source":"ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Edit the main.py file\n* remove avazu, coco, svhn10, ...\n* remove the methods\n* add weight_decay=0 to mnist argomans","metadata":{}},{"cell_type":"code","source":"%%writefile main.py\nfrom typing import *\n# from warnings import simplefilter\n# simplefilter('error')\n\nfrom torch.optim.lr_scheduler import *\nfrom torch import optim\n\nfrom experiment.mnist import ExperimentMNIST\nfrom experiment.imdb import ExperimentIMDb\nfrom experiment.cifar10 import ExperimentCIFAR10\n\nfrom optimizer.adam import Adam\nfrom optimizer.conjugate.conjugate_momentum_adam import ConjugateMomentumAdam\nfrom optimizer.conjugate.coba import CoBA\nfrom optimizer.conjugate.coba2 import CoBA2\n\nOptimizer = Union[Adam, CoBA, ConjugateMomentumAdam]\nOptimizerDict = Dict[str, Tuple[Any, Dict[str, Any]]]\n\n\ndef prepare_optimizers(lr: float, optimizer: str = None, **kwargs) -> OptimizerDict:\n    types = ('HS', 'FR', 'PRP', 'DY', 'CD', 'LS')\n    kw_const = dict(a=1, m=1)\n    # m_dict = dict(m2=1e-2, m3=1e-3, m4=1e-4)\n    m_dict = dict(m0=1)\n    # a_dict = dict(a4=1+1e-4, a5=1+1e-5, a6=1+1e-6, a7=1+1e-7)\n    a_dict = dict(a6=1+1e-6)\n    optimizers = dict(\n        # AMSGrad_ExistingTorch=(optim.Adam, dict(lr=lr, amsgrad=True, **kwargs)),\n        AMSGrad_Existing=(Adam, dict(lr=lr, amsgrad=True, **kwargs)),\n        Adam_Existing=(Adam, dict(lr=lr, amsgrad=False, **kwargs)),\n        # **{f'CoBAMSGrad_{t}_{sm}_{sa}': (CoBA, dict(lr=lr, amsgrad=True, cg_type=t, m=m, a=a, **kwargs))\n        #    for t in types for sm, m in m_dict.items() for sa, a in a_dict.items()},\n        **{f'CoBAMSGrad2_{t}': (CoBA2, dict(lr=lr, amsgrad=True, cg_type=t)) for t in types},\n        # **{f'CoBAMSGrad_{t}(const)': (CoBA, dict(lr=lr, amsgrad=True, cg_type=t, **kw_const)) for t in types},\n        **{f'CoBAMSGrad2_{t}(const)': (CoBA2, dict(lr=lr, amsgrad=True, cg_type=t, **kw_const)) for t in types},\n        # Momentum_Existing=(SGD, dict(lr=lr, momentum=.9, **kwargs)),\n        AdaGrad_Existing=(optim.Adagrad, dict(lr=lr, **kwargs)),\n        RMSProp_Existing=(optim.RMSprop, dict(lr=lr, **kwargs)),\n    )\n    if optimizer:\n        return {optimizer: optimizers[optimizer]}\n    else:\n        return optimizers\n\n\ndef prepare_optimizers_grid_search(lr: float, optimizer: str = None, **kwargs) -> OptimizerDict:\n    types = ('HS', 'FR', 'PRP', 'DY', 'HZ')\n    m_dict = dict(m2=1e-2, m3=1e-3, m4=1e-4)\n    a_dict = dict(a4=1+1e-4, a5=1+1e-5, a6=1+1e-6, a7=1+1e-7)\n    type_dict = dict(\n        HZ=('m2', 'a4'),\n        HS=('m5', 'a5'),\n        FR=('m2', 'a5'),\n        PRP=('m4', 'a4'),\n        DY=('m3', 'a7'),\n    )\n    optimizers = dict(\n        # AMSGrad_ExistingTorch=(optim.Adam, dict(lr=lr, amsgrad=True, **kwargs)),\n        AMSGrad_Existing=(Adam, dict(lr=lr, amsgrad=True, **kwargs)),\n        Adam_Existing=(Adam, dict(lr=lr, amsgrad=False, **kwargs)),\n        **{f'CoBAMSGrad_{t}_{sm}_{sa}': (CoBA,\n                                         dict(lr=lr, amsgrad=True, cg_type=t, m=m_dict[sm], a=a_dict[sa], **kwargs))\n           for t, (sm, sa) in type_dict.items()},\n        # **{f'CoBAMSGrad2_{t}': (CoBA2, dict(lr=lr, amsgrad=True, cg_type=t)) for t in types},\n        # **{f'CoBAMSGrad_{t}(const)': (CoBA, dict(lr=lr, amsgrad=True, cg_type=t, **kw_const)) for t in types},\n        # **{f'CoBAMSGrad2_{t}(const)': (CoBA2, dict(lr=lr, amsgrad=True, cg_type=t, **kw_const)) for t in types},\n        # Momentum_Existing=(SGD, dict(lr=lr, momentum=.9, **kwargs)),\n        AdaGrad_Existing=(optim.Adagrad, dict(lr=lr, **kwargs)),\n        RMSProp_Existing=(optim.RMSprop, dict(lr=lr, **kwargs)),\n    )\n    if optimizer:\n        return {optimizer: optimizers[optimizer]}\n    else:\n        return optimizers\n\n\ndef lr_warm_up(epoch: int, lr: float, t: int = 5, c: float = 1e-2):\n    if epoch <= t:\n        return ((1 - c) * epoch / t + c) * lr\n    else:\n        return lr\n\n\ndef lr_divide(epoch: int, max_epoch: int, lr: float):\n    p = epoch / max_epoch\n    if p < .5:\n        return lr\n    elif p < .75:\n        return lr * 1e-1\n    else:\n        return lr * 1e-2\n\n\ndef lr_warm_up_divide(epoch: int, max_epoch: int, lr: float, t: int = 5, c: float = 1e-2):\n    if epoch <= t:\n        return lr_warm_up(epoch, lr, t, c)\n    else:\n        return lr_divide(epoch, max_epoch, lr)\n\n    \ndef mnist(lr=1e-3, max_epoch=100, weight_decay=.0, batch_size=32, model_name='Perceptron2', use_scheduler=False, **kwargs) -> None:\n    optimizers = prepare_optimizers(lr=lr)\n    scheduler = ReduceLROnPlateau if use_scheduler else None\n    e = ExperimentMNIST(max_epoch=max_epoch, batch_size=batch_size, model_name=model_name, scheduler=scheduler,\n                        **kwargs)\n    e.execute(optimizers)\n\ndef imdb(lr=1e-2, max_epoch=100, weight_decay=.0, batch_size=32, use_scheduler=False, **kwargs) -> None:\n    optimizers = prepare_optimizers(lr=lr)\n    e = ExperimentIMDb(max_epoch=max_epoch, batch_size=batch_size, **kwargs)\n    e.execute(optimizers)\n\ndef cifar10(max_epoch=300, lr=1e-3, weight_decay=0, batch_size=128, model_name='DenseNetBC24', num_workers=0,\n            optimizer=None, use_scheduler=False, **kwargs) -> None:\n    scheduler = LambdaLR if use_scheduler else None\n    kw_scheduler = dict(lr_lambda=lambda epoch: lr_warm_up(epoch, lr))\n    optimizers = prepare_optimizers(lr=lr, optimizer=optimizer, weight_decay=weight_decay)\n    e = ExperimentCIFAR10(max_epoch=max_epoch, batch_size=batch_size, model_name=model_name,\n                          kw_loader=dict(num_workers=num_workers), scheduler=scheduler, kw_scheduler=kw_scheduler, \n                          **kwargs)\n    e(optimizers)\n\n\nif __name__ == '__main__':\n    from argparse import ArgumentParser\n    p = ArgumentParser()\n    p.add_argument('-e', '--experiment')\n    p.add_argument('-m', '--model_name')\n    p.add_argument('-d', '--data_dir', default='dataset/data')\n    p.add_argument('-me', '--max_epoch', type=int)\n    p.add_argument('-bs', '--batch_size', type=int)\n    p.add_argument('--lr', type=float)\n    p.add_argument('--device')\n    p.add_argument('-nw', '--num_workers', type=int)\n    p.add_argument('-us', '--use_scheduler', action='store_true')\n    p.add_argument('-o', '--optimizer', default=None)\n    p.add_argument('-wd', '--weight_decay', default=0, type=float)\n    args = p.parse_args()\n\n    experiment = args.experiment\n    kw = {k: v for k, v in dict(**args.__dict__).items() if k != 'experiment' and v is not None}\n    print(kw)\n    d: Dict[str, Callable] = dict(\n        MNIST=mnist,\n        IMDb=imdb,\n        CIFAR10=cifar10    \n    )\n    d[experiment](**kw)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Edit the requirements.txt\n* torchtext==0.6.0","metadata":{}},{"cell_type":"markdown","source":"How I loaded the requirements.txt:\nI used the magic command: %load requirements.txt\nand I used the magic command %%writefile requirements.txt at the first of the cell to write to that file.","metadata":{}},{"cell_type":"code","source":"%%writefile requirements.txt\npip\npycocotools\ngoogle-api-python-client\ngoogle-auth-oauthlib\nmatplotlib\nnumpy\npandas\nscikit-learn\ntorch\ntorchfm\ntorchtext==0.6.0\ntorchvision\ntqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This line will install the requirements of the project","metadata":{}},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This cell of code is in bash script format, it will remove some extra files in the experiment folder and will go back after that","metadata":{}},{"cell_type":"code","source":"%%bash\ncd experiment\nrm avazu.py stl10.py coco.py svhn.py\ncd ..","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is runner of the main.py and it will run the main file with MNIST dataset and resnet model in 10 epoches, through cuda gpu","metadata":{}},{"cell_type":"code","source":"!python main.py -e MNIST -m resnet -me 100 --device \"cuda:0\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls result/MNIST/resnet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%bash\ncd ..\nzip -r coba /kaggle/working/coba\ncd coba\nzip -r result /kaggle/working/coba/result","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}